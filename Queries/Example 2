
-- Set the context.
USE ROLE sysadmin;
USE bike.PUBLIC;
USE WAREHOUSE loading_wh;

---------------------------------
-- WORKING WITH SEMI-STRUCTURED DATA
---------------------------------

-- Create a table to store the weather data (JSON file).
CREATE TABLE weather_json (v variant);

-- Create a stage that specifies the location of the S3 bucket.
-- This is public bucket, so no credentials are required.
CREATE STAGE weather_stage
    url = 's3://weather-files/';

-- List the files in the stage.
-- Remember, our trip data is from mid-2013 to mid-2018.
-- This weather data is from late-2016 to mid-2019.
LIST @weather_stage;

-- Load the staged data into the table.
COPY INTO weather_json
FROM @weather_stage
file_format = (type = json
               strip_outer_array = true); --Pull out brackets.
               
-- Preview 10 records.
SELECT * FROM weather_json LIMIT 10;
_____

-- Create a view that will structure the semi-structured data.
-- In this case, a VIEW allows the data to be queried like a table.
-- The DOT NOTATION allows us to pull data lower in the hierarchy.
CREATE OR REPLACE VIEW weather_view AS
SELECT
    v:obsTime::timestamp as observation_time,
    v:station::string as station_id,
    v:name::string as city_name,
    v:country::string as country,
    v:latitude::float as city_lat,
    v:longitude::float as city_lon,
    v:weatherCondition::string as weather_conditions,
    v:coco::int as weather_conditions_code,
    v:temp::float as temp,
    v:prcp::float as rain,
    v:tsun::float as tsun,
    v:wdir::float as wind_dir,
    v:wspd::float as wind_speed,
    v:dwpt::float as dew_point,
    v:rhum::float as relative_humidity,
    v:pres::float as pressure
FROM
    weather_json
WHERE
    station_id = '72502';

-- Preview 10 records of the VIEW.
SELECT * FROM weather_view LIMIT 10;

-- Examine a month of weather data.
-- This includes hourly measurements during January 2018.
SELECT *
FROM weather_view
WHERE DATE_TRUNC('_____',observation_time) = '2018-01-01';

---------------------------------
-- USING ADVANCED SQL
---------------------------------

-- First, we'll switch to the QUERY_WH as we explore the data.
USE WAREHOUSE query_wh;

-- Add the queries discussed in class.

--1. Pull row data, create a new field, and provide row # for the trip data (starttime and start_station)
SELECT starttime, start_station_name,
    RANK () OVER (ORDER BY starttime) TRIP_NUMBER
FROM trips
ORDER BY starttime
LIMIT 100;

--2.
SELECT starttime, start_station_name, tripduration, AVG(tripduration) AS average_trip_duration
FROM trips
ORDER BY average_trip_duration
LIMIT 100;

--3
SELECT 
    EXTRACT (year, starttime) AS year,
    EXTRACT (month, starttime) AS month,
    COUNT (*) AS trips,
    SUM (trips) OVER (ORDER BY year, month
                      ROWS UNBOUNDED PRECEDING) AS cumulative_trips
FROM trips
GROUP BY year, month
ORDER BY year, month
LIMIT 100;

--4
SELECT 
    EXTRACT (year, starttime) AS year,
    EXTRACT (month, starttime) AS month,
    COUNT (*) AS trips,
    LAG (trips) OVER (ORDER BY year, month) AS previous_month,
    (trips-previous_month) AS change_in_trips
FROM trips
GROUP BY year, month
ORDER BY year, month
LIMIT 100;

--5. When customers were riding bikes, what weather was it?
SELECT w.weather_conditions, t.starttime
FROM trips t
INNER JOIN weather_view w
ON DATE_TRUNC('hour', t.starttime) = DATE_TRUNC('hour', w.observation_time)
ORDER BY t.starttime
LIMIT 200;

--6. SELECT w.weather_conditions, t.starttime
SELECT COUNT (*) t.trips AS trips_by_weather, w.weather_conditions, t.starttime
FROM trips t
INNER JOIN weather_view w
ON DATE_TRUNC('hour', t.starttime) = DATE_TRUNC('hour', w.observation_time)
GROUP BY weather_conditions
ORDER BY t.starttime
LIMIT 200;

--7
WITH monthly_rides AS (
    SELECT
        DATE_TRUNC('month', starttime) AS month,
        COUNT(*) AS num_rides
    FROM trips
    GROUP BY month
    ORDER BY month
),
monthly_temp AS (
    SELECT
        DATE_TRUNC('month', observation_time) AS month,
        ROUND(AVG(w.temp), 1) AS avg_temp
    FROM weather_view w
    GROUP BY month
    ORDER BY month
)
SELECT 
    EXTRACT(year, r.month) AS year,
    EXTRACT(month, r.month) AS month,
    r.num_rides,
    t.avg_temp
FROM monthly_rides r
INNER JOIN monthly_temp t
ON r.month = t.month
ORDER BY year, month;

--8. What is our monthly grow rate in retars. Is there seasonality?
SELECT DATE_TRUNC('MONTH', starttime) AS month,
       COUNT(*) AS current_rentals,
       LAG(current_rentals) OVER (ORDER BY month) AS previous_rentals,
       ROUND((current_rentals-previous_rentals)/previous_rentals,2) AS growth_rate
FROM trips
GROUP BY month
ORDER BY month;

--9. For each bike for each year what is the usage by year, annual bike hours and using a window total bike time

    
---------------------------------
-- TIME TRAVEL
---------------------------------

-- Enables historical data access.
-- The default is 24 hours.
-- Common uses include restoring or backing up data.

-- DROP & UNDROP TABLES
DROP TABLE weather_json;
SELECT * FROM weather_json LIMIT 10;
UNDROP TABLE weather_json;
SELECT * FROM weather_json LIMIT 10;

-- ROLLBACK TABLES
-- Count rides by starting station.
SELECT start_station_name, COUNT(*) FROM trips GROUP BY start_station_name;
-- Set all starting stations to "oops".
UPDATE trips SET start_station_name = 'oops';
-- Recount rides by starting station.
SELECT start_station_name, COUNT(*) FROM trips GROUP BY start_station_name;
-- Option #1: Find the query ID in the console.
-- Option #2: Find the query ID using SQL.
SELECT * FROM table(information_schema.query_history_by_session (result_limit=>10));
-- Restore the table using the query ID.
CREATE OR REPLACE TABLE trips AS
(SELECT * FROM trips before (statement => '01bacee6-0002-82e5-0007-2e3e0008a092'));
-- RECOUNT rides by starting station. (It should be restored.)
SELECT start_station_name, COUNT(*) FROM trips GROUP BY start_station_name;

---------------------------------
-- ACCESS CONTROL
---------------------------------

-- Switch roles to see access (nothing) then switch back to grant.
USE ROLE junior_analyst;
SELECT * FROM trips LIMIT 10;
USE ROLE sysadmin;
SELECT * FROM trips LIMIT 10;

-- Recheck access levels, then switch back.
USE ROLE junior_analyst;
SELECT * FROM trips LIMIT 10;
USE ROLE sysadmin;
SELECT * FROM trips LIMIT 10;

-- If you need to try again:
USE ROLE accountadmin;
DROP ROLE junior_analyst;
USE ROLE sysadmin;

---------------------------------
-- RESETTING THE ENVIRONMENT
---------------------------------

-- Reset your environment.
USE ROLE accountadmin;
DROP SHARE IF EXISTS crunchbase_company_data;
DROP ROLE IF EXISTS junior_analyst;
DROP DATABASE IF EXISTS bike;
DROP DATABASE IF EXISTS crunchbase_company_data;
-- Delete Crunchbase worksheet.

USE ROLE sysadmin;
USE WAREHOUSE loading_wh;

---------------------------------
-- CREATE THE DATABASE & TABLES
---------------------------------

-- Create a new database.
CREATE DATABASE IF NOT EXISTS tickets;

-- Create the tables.
CREATE OR REPLACE TABLE listings (
    listid integer not null,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp
);

CREATE OR REPLACE TABLE sales (
	salesid integer not null,
	listid integer not null,
	sellerid integer not null,
	buyerid integer not null,
	eventid integer not null,
	dateid smallint not null,
	qtysold smallint not null,
	pricepaid decimal(8,2),
	commission decimal(8,2),
	saletime timestamp
);

CREATE OR REPLACE TABLE users (
	userid integer not null,
	username char(8),
	firstname varchar(30),
	lastname varchar(30),
	city varchar(30),
	state char(2),
	email varchar(100),
	phone char(14),
	likesports boolean,
	liketheatre boolean,
	likeconcerts boolean,
	likejazz boolean,
	likeclassical boolean,
	likeopera boolean,
	likerock boolean,
	likevegas boolean,
	likebroadway boolean,
	likemusicals boolean
);

CREATE OR REPLACE TABLE venues (
	venueid smallint not null,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer
);
   
CREATE OR REPLACE TABLE categories (
	catid smallint not null,
	catgroup varchar(10),
	catname varchar(10),
	catdesc varchar(50)
);

CREATE OR REPLACE TABLE dates (
	dateid smallint not null,
	caldate date not null,
	day character(3) not null,
	week smallint not null,
	month character(5) not null,
	qtr character(5) not null,
	year smallint not null,
	holiday boolean default('N')
);

CREATE OR REPLACE TABLE events (
	eventid integer not null,
	venueid smallint not null,
	catid smallint not null,
	dateid smallint not null,
	eventname varchar(200),
	starttime timestamp
);


----------------------------------
-- LOAD ONE OF THE TABLES
----------------------------------
    -- Create a STAGE that specifies the location of the S3 bucket.
-- STAGES are locations where files are stored for loading or unloading.
-- This is public S3 bucket, so no credentials are required.
CREATE OR REPLACE STAGE ticket_files
    url = 's3://746-tickets';

-- List the files in the stage. (It does NOT show delimiters.)
LIST @ticket_files;

-- Review the properties of the stage. (Delimiters are noted.)
DESCRIBE STAGE ticket_files;

-- Query some of the files in the staging area.
SELECT $1 FROM @ticket_files/venues_pipe.txt LIMIT 20;
SELECT $1 FROM @ticket_files/sales_tab.txt LIMIT 20;

-- Create a FILE FORMAT that matches the data structure.
-- It describes what the staged files look like.

CREATE OR REPLACE FILE FORMAT tickets_tab
    type='csv'
    field_delimiter = '\t' -- columns separated by commas (default)
    record_delimiter = '\n' -- rows separated by newlines (default)
    skip_header = 0 -- does not skip lines in the file (default)
    trim_space = true -- does not remove white space from fields (default)
    null_if = ('')
    date_format = 'auto'
    timestamp_format = 'auto'
    error_on_column_count_mismatch = true
;
    
-- Load data into the sales table.
COPY INTO sales -- destination table
  FROM @ticket_files/sales_tab -- source file(s) 
  file_format = tickets_tab;

SELECT * FROM sales LIMIT 10;

----------------------------------
-- ANSWER BUSINESS QUESTIONS
----------------------------------

-- Switch to the query warehouse.
USE WAREHOUSE query_wh;

-- ADD OTHER QUERIES FROM CLASS.
--What time period does this data cover?
SELECT 
    MIN(saletime) AS earliest_sale_date,
    MAX(saletime) AS latest_sale_date
FROM sales;

--How many tickets were sold?  How much revenue was generated?
SELECT 
    SUM(qtysold) AS "Total Ticker Sold",
    SUM(commission) AS "Total Commission Earned"
FROM sales;

--What was the average price paid per ticket?
SELECT ROUND(SUM(pricepaid)/SUM(qtysold),2) AS "Price Paid Per Ticket"
FROM sales;

--How many buyers?
SELECT COUNT (DISTINCT buyerid) AS "# of Buyers"
    FROM sales;

--How many sellers also bought tickets?
SELECT COUNT (DISTINCT sellerid) AS "# of Buyers"
FROM sales
WHERE sellerid IN (SELECT buyerid
                    FROM sales);

--What was the % change of quarter vs. quarter sales?
SELECT 
    EXTRACT (YEAR FROM saletime) AS year,
    EXTRACT(QUARTER FROM saletime) AS quarter,
    ROUND(sum(pricepaid) / 1000000,2) AS current_sales_mil,
    LAG(current_sales_mil) OVER (ORDER BY year, quarter) AS previous_sales_mil,
    ROUND((current_sales_mil - previous_sales_mil)/previous_sales_mil)*100,2 AS pct_change
FROM sales
GROUP BY year, quarter;

---Calculate cumulative commission revenue by month in millions of dollars.  What was cumulative revenue at the end of September?
CREATE OR REPLACE TABLE sales (
	salesid integer not null,
	listid integer not null,
	sellerid integer not null,
	buyerid integer not null,
	eventid integer not null,
	dateid smallint not null,
	qtysold smallint not null,
	pricepaid decimal(8,2),
	commission decimal(8,2),
	saletime timestamp
);


SELECT
    DATE_TRUNC('month', saletime) AS month,
    SUM(commission) / 1000000 AS monthly_commission,
    SUM(SUM(commission)) OVER (ORDER BY DATE_TRUNC('month',saletime))
FROM sales
WHERE DATE_TRUNC('month', saletime) <= '2024-09-30'
GROUP BY DATE_TRUNC ('month', saletime)
ORDER BY month;

-- Calculate a 7-day rolling average of tickets sold.  On June 19th, what was the average?
select date_trunc(day,saletime) AS day,
SUM(qtysold) AS tickets_sold,
AVG(tickets_sold) OVER (ORDER BY day
                        ROWS BETWEEN 6 PRECEDING
                        AND CURRENT ROW)
                        AS seven_day_avg

--Calculate buyer spend by event. dditionally, in two other columns, show the buyer's total spend and the % of their total spend on the listed event.  For our largest buyer (#4303), what % of their spend was on event #7851?  (They spent the most money on that event. The concert was for a band called "Live".)
SELECT
    buyerid,
    eventid,
    SUM(pricepaid) AS buyer_event_spend,
    SUM(buyer_event_spend) OVER (PARTITION BY buyerid) AS buyer_spend,
    ROUND(buyer_event_spend/buyer_spend,3)*100

                        

----------------------------------
-- EVALUTE QUERY PERFORMANCE
----------------------------------

-- QUERY PROFILE
-- Provides execution details for a query.
-- Includes graphs/stats to understand query components.

-- Temporarily turn off cached results. (For experimentation.)
ALTER SESSION SET USED_CACHED_RESULT = FALSE;

-- Execute a query, which we'll review in the QUERY PROFILE.
-- For each event, it shows tickets sold and averge ticket price.
SELECT
    eventid,
    SUM(qtysold) AS tickets_sold,
    ROUND(AVG(pricepaid/qtysold)) AS avg_price_per_ticket
FROM sales
GROUP BY eventid
HAVING SUM(qtysold) >= 25
ORDER BY SUM(qtysold) DESC;

-- ANALYZE THE QUERY PROFILE
-- In the query details, use the ellipse to view the profile.
-- QUERY DETAILS TAB - shows summary information
-- QUERY PROFILE TAB - see the SLIDES for more detail


-- METADATA CACHE
-- Snowflake stores COUNT, MAX, MIN, etc. in metadata cache.
-- When needed, it can avoid table scans and calculations.
-- It simply pulls those metrics from metadata cache.
SELECT MAX(pricepaid), MIN(pricepaid) FROM sales;
-- Then, view the QUERY PROFILE.

-- Turn cache reuse back on.
ALTER SESSION SET USE_CASH_RESULT = TRUE;

-- Rerun our previous query (twice if needed) to show results cache.
SELECT
    eventid,
    SUM(qtysold) AS tickets_sold,
    ROUND(AVG(pricepaid/qtysold)) AS avg_price_per_ticket
FROM sales
GROUP BY eventid
HAVING SUM(qtysold) >= 25
ORDER BY SUM(qtysold) DESC;

USE tickets.public;
USE WAREHOUSE loading_wh;

LIST @ticket_files;

-- Create a format that matches our pipe files.
CREATE OR REPLACE FILE FORMAT tickets_pipe
    type='csv'
    field_delimiter = '|'
    record_delimiter = '\n'
    skip_header = 0
    null_if = ('')
;

-----------
-- LISTINGS
-----------

-- Preview the listings file(s) on staging.
SELECT $1 FROM @ticket_files/listings LIMIT 100;

-- Load data into the listings table.
COPY INTO listings
  FROM @ticket_files/listings
  file_format = tickets_pipe
;

-- Preview the listings table.
SELECT * FROM listings LIMIT 100;


-----------
-- DATES
-----------

-- Preview the date file(s) on staging.
SELECT $1 FROM @ticket_files/dates LIMIT 100;

-- Load data into the dates table.
COPY INTO dates
  FROM @ticket_files/dates
  file_format = tickets_pipe;

-- Preview the dates table.
SELECT * FROM dates LIMIT 100;

ALTER TABLE dates
ADD COLUMN season string;

-----------
-- EVENTS
-----------

-- Preview the events file(s) on staging.
SELECT $1 FROM @ticket_files/events LIMIT 100;

-- Load data into the events table.
COPY INTO events
  FROM @ticket_files/events
  file_format = tickets_pipe
  ON_ERROR = 'CONTINUE';

-- Preview the events table.
SELECT * FROM events LIMIT 100;

-----------
-- USERS
-----------

-- Preview the user file(s) on staging.
SELECT $1 FROM @ticket_files/users LIMIT 100;

-- Load data into the users table.
COPY INTO users
  FROM @ticket_files/users
  file_format = (field_delimiter = '|'
                 trim_space = TRUE);

-- Preview the users table.
SELECT * FROM users LIMIT 100;

-----------
-- CATEGORIES
-----------

-- Query the category files(s) on staging.
SELECT $1 FROM @ticket_files/categories LIMIT 100; --This is a JSON file

-- Create a table to store the categories data (JSON).
CREATE OR REPLACE TABLE categories_json (v variant);

-- Load data into the categories table.
COPY INTO categories_json
  FROM @ticket_files/categories
  file_format = (type = 'json'
                 strip_outer_array = TRUE)
                 ;

-- Preview the categories data (JSON).
SELECT * FROM categories_json;

-- Create a view to structure the data.
CREATE OR REPLACE VIEW categories_view AS
SELECT
    v:catid::integer AS catid,
    v:catgroup::string AS catgroup,
    v:catname::string AS catname,
    v:catdesc::string AS catdesc
FROM categories_json;

-- Preview data in the categories view.
SELECT * FROM categories_view;

-----------
-- VENUES
-----------

-- Query the venues files(s) on staging.
SELECT $1 FROM @ticket_files/venues LIMIT 200;

-- Load data into the venues table.
-- This will generate an ERROR that needs fixing.
COPY INTO venues
  FROM @ticket_files/venues
  FILE_FORMAT = (field_delimiter = ':', null_if = ('NULL'))
  ;

-- Preview data in the venues table.
SELECT * FROM venues LIMIT 100;

----------------------------------
-- UNLOADING DATA
----------------------------------

-- Instead of loading data into a table...
-- We will UNLOAD DATA to a FILE, and put that on a STAGE.
-- This is different than just exporting query results.
-- Oftentimes, you will be exporting to an external stage like S3.

----------------------------------
----------------------------------

-- UNLOAD TO AN INTERNAL, NAMED STAGE

-- Create a named stage.
CREATE OR REPLACE STAGE tickets_named_stage;

-- Copy the results from a query to the stage.
COPY INTO @tickets_named_stage FROM
    (SELECT * FROM dates);

-- Examine the files in the NAMED STAGE.
LIST @tickets_named_stage;

-- Verify there is data in the file.
SELECT $1, $2, $3 FROM @_____/data_0_0_0.csv.gz LIMIT 10;

-- Remove the file from staging, and drop the stage.
REMOVE @tickets_named_stage/data_0_0_0.csv.gz;
LIST @tickets_named_stage;
DROP STAGE tickets_named_stage;

----------------------------------

-- UNLOAD TO AN EXTERNAL, NAMED STAGE (AWS S3)

-- Create the stage, which points to S3.
-- This won't work as we haven't setup an integration with S3.
CREATE OR REPLACE STAGE stage_tickets_s3
    URL = 's3://746-tickets'
    STORAGE_INTEGRATION = s3_int;

----------------------------------
-- ANSWERING BUSINESS QUESTIONS
----------------------------------

-- Switch to the query warehouse.
USE WAREHOUSE query_wh;

-- Revisit what the tables look like.
SELECT * FROM listings LIMIT 10;
SELECT * FROM sales LIMIT 10;
SELECT * FROM venues LIMIT 10;
SELECT * FROM events LIMIT 10;
SELECT * FROM categories_view LIMIT 10;
SELECT * FROM dates LIMIT 10;
SELECT * FROM users LIMIT 10;

-- Answer the business questions from class.

---What was the highest-selling individual event?

SELECT e.eventid, e.eventname, e.starttime, ROUND(SUM(S.pricepaid)) AS total_sales_usd
FROM sales s
LEFT JOIN events e
ON s.eventid = e.eventid
GROUP BY e.eventid, e.eventname, e.starttime
ORDER BY total_sales_usd DESC
LIMIT 10;

---Who bought the most tickets?
SELECT s.buyerid, u.firstname, u.lastname, SUM(s.qtysold)
FROM users u
LEFT JOIN sales s
ON u.

---Re-create the query
SELECT 
    d.caldate AS day, 
    d.season,
    SUM(s.qtysold) AS ticket_sold,
    SUM(ticket_sold) OVER (ORDER BY d.caldate) AS cumulative_tickets,
    SUM(ticket_sold) OVER (PARTITION BY d.season) AS tickets_season
FROM sales s
INNER JOIN dates d
ON s.dateid = d.dateid
GROUP BY d.caldate, d.season
ORDER BY d.caldate;

---Re-crate query two
SELECT c.catname, SUM(s.qtysold) AS listed_tickets_total
FROM category c
INNER JOIN sales s


